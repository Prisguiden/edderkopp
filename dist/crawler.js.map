{"version":3,"sources":["../src/crawler.js"],"names":[],"mappings":";;;;;;;;AAAA;;;;AACA;;;;AACA;;;;AACA;;;;AACA;;;;AACA;;;;AACA;;;;AACA;;;;;;;;;AAUI,oBAAY,IAAZ,EAAkB;AAAA;;AAAA,aANlB,KAMkB,GANV,EAMU;AAAA,aALlB,QAKkB,GALP,CAKO;AAAA,aAJlB,QAIkB,GAJP,CAIO;AAAA,aAFlB,SAEkB,GAFN,kJAEM;;AACd,aAAK,KAAL,GAAa,qBAAb;AACA,aAAK,GAAL,GAAW,oBAAQ,KAAK,GAAb,CAAX;AACA,aAAK,GAAL,CAAS,IAAT,GAAgB,KAAK,GAAL,CAAS,QAAT,KAAsB,KAAtB,GAA8B,KAAK,GAAL,CAAS,QAAT,EAA9C;AACA,aAAK,KAAL,GAAa,KAAK,KAAlB;AACA,YAAI,QAAQ,KAAK,KAAjB,EAAwB;AACpB,iBAAK,IAAI,IAAT,IAAiB,KAAK,KAAtB,EAA6B;AACzB,qBAAK,IAAL,IAAa,KAAK,KAAL,CAAW,IAAX,CAAb;AACH;AACJ;AACJ;;;;;;;gCAEO;AAAA;;AACJ,mBAAO,IAAI,OAAJ,CAAY,UAAC,OAAD,EAAU,MAAV,EAAqB;;;AAGpC,oBAAM,aAAa,MAAK,GAAL,CAAS,IAAT,GAAgB,aAAnC;AACA,oBAAM,gBAAgB,aAAG,YAAH,CAAgB,YAAY,oBAA5B,EAAkD,QAAlD,EAAtB,C;AACA,oBAAI,aAAJ,EAAmB;AACf,0BAAK,MAAL,GAAc,4BAAa,UAAb,EAAyB,aAAzB,CAAd;;;AAGA,wBAAM,QAAQ,MAAK,MAAL,CAAY,aAAZ,EAAd;AACA,wBAAI,SAAS,QAAQ,MAAK,KAA1B,EAAiC;AAC7B,8BAAK,KAAL,GAAa,KAAb;AACH;;;AAGD,wBAAI,CAAC,MAAK,MAAL,CAAY,SAAZ,CAAsB,MAAK,GAAL,CAAS,IAA/B,EAAqC,UAArC,CAAL,EAAuD;AACnD,+BAAO,wBAAP;AACA;AACH;AACJ;;;AAGD,sBAAK,KAAL,GAAa,oBAAU,MAAK,QAAf,EAAyB,MAAK,QAA9B,CAAb;AACA,sBAAK,KAAL,CAAW,GAAX,CAAe,MAAK,GAAL,CAAS,QAAT,EAAf;;;AAGA,sBAAK,KAAL,CAAW,GAAX,CAAe,MAAK,GAAL,CAAS,QAAT,EAAf;;;AAGA,sBAAK,MAAL,GAAc,IAAd,CAAmB,YAAM;AACrB;AACH,iBAFD;AAGH,aAhCM,CAAP;AAiCH;;;;;;;iCAIQ;AAAA;;AACL,mBAAO,IAAI,OAAJ,CAAY,UAAC,OAAD,EAAU,MAAV,EAAqB;AACpC,oBAAM,MAAM,OAAK,KAAL,CAAW,GAAX,EAAZ;AACA,oBAAI,GAAJ,EAAS;AACL,kCAAI,OAAJ,CAAY,yBAAyB,GAArC;AACA,4CAAS,GAAT,EAAc,IAAd,CAAmB,UAAC,GAAD,EAAS;AACxB,sCAAI,KAAJ,CAAU,qBAAqB,IAAI,IAAzB,IAAiC,IAAI,IAAJ,GAAW,SAAX,GAAuB,iBAAxD,CAAV;AACA,sCAAI,KAAJ,CAAU,qBAAqB,IAAI,IAAnC;;;AAGA,4BAAM,SAAS,qBAAW,IAAI,IAAf,CAAf;;;AAGA,4BAAI,QAAQ,OAAO,QAAP,EAAZ;AACA,sCAAI,KAAJ,CAAU,wBAAwB,MAAM,MAA9B,GAAuC,QAAjD;AACA,gCAAQ,OAAK,cAAL,CAAoB,KAApB,CAAR;AACA,sCAAI,KAAJ,CAAU,qBAAqB,MAAM,MAA3B,GAAoC,YAA9C;AACA,4BAAI,MAAM,MAAV,EAAkB;AACd,mCAAK,KAAL,CAAW,GAAX,CAAe,KAAf;AACH;;;;;;;AAOD,4BAAI,CAAC,OAAK,KAAL,CAAW,OAAX,EAAL,EAA2B;AACvB,0CAAI,KAAJ,CAAU,qBAAqB,OAAK,KAA1B,GAAkC,IAA5C;AACA,uCAAW,YAAM;AACb,wCAAQ,IAAR;AACH,6BAFD,EAEG,OAAK,KAAL,GAAa,IAFhB;AAGH,yBALD,MAKO;AACH,oCAAQ,KAAR;AACH;AACJ,qBA7BD,EA6BG,KA7BH,CA6BS,UAAC,CAAD,EAAO;AACZ,sCAAI,KAAJ,CAAU,CAAV;AACH,qBA/BD;AAgCH;AACJ,aArCM,EAsCN,IAtCM,CAsCD,UAAC,YAAD,EAAkB;AACpB,oBAAI,YAAJ,EAAkB;AACd,2BAAO,OAAK,MAAL,EAAP;AACH,iBAFD,MAEO;AACH;AACH;AACJ,aA5CM,EA6CN,KA7CM,CA6CA,UAAC,CAAD,EAAO;AACV,8BAAI,KAAJ,CAAU,CAAV;AACH,aA/CM,CAAP;AAgDH;;;uCAGc,K,EAAO;AAClB,gBAAI,SAAS,EAAb;AACA,gBAAI,MAAM,qBAAV;AAFkB;AAAA;AAAA;;AAAA;AAGlB,qCAAc,KAAd,8HAAqB;AAAA,wBAAZ,CAAY;;;AAEjB,wBAAI,IAAJ,CAAS,CAAT;;;AAGA,wBAAI,IAAI,QAAJ,MAAkB,IAAI,QAAJ,GAAe,OAAf,CAAuB,MAAvB,MAAmC,CAAzD,EAA4D;AACxD,sCAAI,KAAJ,CAAU,4CAA4C,IAAtD;AACA;AACH;;;AAGD,wBAAI,CAAC,IAAI,IAAJ,EAAL,EAAiB;AACb,4BAAI,IAAJ,CAAS,KAAK,GAAL,CAAS,IAAT,EAAT;AACH,qBAFD,MAEO,IAAI,IAAI,IAAJ,MAAc,KAAK,GAAL,CAAS,IAAT,EAAlB,EAAmC;AACtC,sCAAI,KAAJ,CAAU,sCAAsC,IAAhD;AACA;AACH;;;AAGD,wBAAI,QAAJ,CAAa,KAAK,GAAL,CAAS,QAAT,EAAb;;;AAGA,wBAAI,SAAJ;;;AAGA,wBAAI,IAAJ,CAAS,IAAI,IAAJ,GAAW,OAAX,CAAmB,KAAnB,EAA0B,EAA1B,CAAT;;;AAGA,wBAAI,IAAJ,CAAS,EAAT;;;AAGA,wBAAI,OAAM,IAAI,QAAJ,EAAV;;;AAGA,wBAAI,KAAK,KAAL,CAAW,GAAX,CAAe,IAAf,CAAJ,EAAyB;AACrB,sCAAI,KAAJ,CAAU,sCAAsC,IAAhD;AACA;AACH,qBAHD,MAGO;AACH,6BAAK,KAAL,CAAW,GAAX,CAAe,IAAf;AACH;;;AAGD,wBAAI,IAAI,MAAJ,GAAa,KAAb,CAAmB,KAAK,SAAxB,MAAuC,IAA3C,EAAiD;AAC7C,sCAAI,KAAJ,CAAU,4BAA4B,IAAtC;AACA;AACH;;;AAGD,wBAAI,KAAK,MAAL,IAAe,KAAK,MAAL,CAAY,YAAZ,CAAyB,IAAzB,EAA8B,UAA9B,CAAnB,EAA8D;AAC1D,sCAAI,KAAJ,CAAU,gDAAgD,IAA1D;AACA;AACH;;;AAGD,wBAAI,KAAK,IAAL,CAAU,IAAd,EAAoB;AAChB,4BAAI,OAAO,KAAX;AADgB;AAAA;AAAA;;AAAA;AAEhB,kDAAc,KAAK,IAAL,CAAU,IAAxB,mIAA8B;AAAA,oCAArB,CAAqB;;AAC1B,oCAAI,IAAI,IAAJ,GAAW,OAAX,CAAmB,CAAnB,MAA0B,CAA9B,EAAiC;AAC7B,kDAAI,KAAJ,CAAU,4BAA4B,IAAtC;AACA,2CAAO,IAAP;AACA;AACH;AACJ;AARe;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAShB,4BAAI,IAAJ,EAAU;AAAE;AAAW;AAC1B;AACD,2BAAO,IAAP,CAAY,IAAZ;AACH;AArEiB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AAqEjB;AACD,mBAAO,MAAP;AACH","file":"crawler.js","sourcesContent":["import fs from 'fs';\nimport URI from 'urijs';\nimport robotsParser from 'robots-parser';\nimport Parser from \"./parser\";\nimport log from './log';\nimport Queue from './queue';\nimport download from \"./download\";\nimport Cache from './cache';\n\nexport default class {\n\n    delay = 60; // sec\n    maxPages = 5;\n    maxDepth = 2;\n    // Skip some common filetypes 'cause you never know whats out there (http://fileinfo.com/filetypes/common)\n    skipFiles = /jpg|jpeg|png|gif|bmp|tif|tiff|svg|pdf|wav|mpa|mp3|avi|flv|m4v|mov|mp4|mpg|swf|wmv|tar|gz|zip|rar|pkg|7z|xls|doc|log|odt|rtf|txt|exe|jar|com|bat/i;\n\n    constructor(conf) {\n        this.cache = new Cache();\n        this.uri = new URI(conf.url);\n        this.uri.root = this.uri.protocol() + '://' + this.uri.hostname()\n        this.pages = conf.pages;\n        if (conf && conf.crawl) {\n            for (let prop in conf.crawl) {\n                this[prop] = conf.crawl[prop];\n            }\n        }\n    }\n\n    start() {\n        return new Promise((fulfill, reject) => {\n\n            // robots.txt\n            const robotsFile = this.uri.root + '/robots.txt';\n            const robotsContent = fs.readFileSync(__dirname + '/../tmp/robots.txt').toString(); // tmp\n            if (robotsContent) {\n                this.robots = robotsParser(robotsFile, robotsContent);\n\n                // If robots spesifies delay and it is greater than ours, respect it!\n                const delay = this.robots.getCrawlDelay();\n                if (delay && delay > this.delay) {\n                    this.delay = delay;\n                }\n\n                // Makes sure we are wanted\n                if (!this.robots.isAllowed(this.uri.root, USER_AGENT)) {\n                    reject('Stopped by robots.txt!');\n                    return;\n                }\n            }\n\n            // Init queue\n            this.queue = new Queue(this.maxPages, this.maxDepth);\n            this.queue.add(this.uri.toString());\n\n            // Cache main url\n            this.cache.set(this.uri.toString());\n\n            // Start crawling from queue\n            this._crawl().then(() => {\n                fulfill();\n            });\n        });\n    }\n\n    // Recursively crawl urls from queue\n    // Promise pattern: https://gist.github.com/fractalf/c0eb369373d8fb1242ebb537e20e4794\n    _crawl() {\n        return new Promise((fulfill, reject) => {\n            const url = this.queue.get();\n            if (url) {\n                log.verbose('[crawler] download: ' + url);\n                download(url).then((res) => {\n                    log.debug('[crawler] size: ' + res.size + (res.gzip ? ' (gzip)' : ' (uncompressed)'));\n                    log.debug('[crawler] time: ' + res.time);\n\n                    // Parse html\n                    const parser = new Parser(res.html);\n\n                    // Get and validate all links and add to queue\n                    let links = parser.getLinks();\n                    log.debug('[crawler] validate ' + links.length + ' links');\n                    links = this._validateLinks(links);\n                    log.debug('[crawler] found ' + links.length + ' new links');\n                    if (links.length) {\n                        this.queue.add(links);\n                    }\n\n                    // #########################################################################\n                    // TODO: Need to do something with the html document at hand!\n                    // #########################################################################\n\n                    // Crawl next in queue\n                    if (!this.queue.isEmpty()) {\n                        log.debug('[crawler] sleep ' + this.delay + ' s');\n                        setTimeout(() => {\n                            fulfill(true);\n                        }, this.delay * 1000);\n                    } else {\n                        fulfill(false);\n                    }\n                }).catch((e) => {\n                    log.error(e);\n                })\n            }\n        })\n        .then((keepCrawling) => {\n            if (keepCrawling) {\n                return this._crawl();\n            } else {\n                return;\n            }\n        })\n        .catch((e) => {\n            log.error(e);\n        });\n    }\n\n\n    _validateLinks(links) {\n        let result = [];\n        let uri = new URI();\n        for (let v of links) {\n            // Populate URI object\n            uri.href(v);\n\n            // Skip protocols other than http(s)\n            if (uri.protocol() && uri.protocol().indexOf('http') !== 0) {\n                log.silly('[crawler] Skip: unsupported protocol - ' + url);\n                continue;\n            }\n\n            // Set host and skip different hosts\n            if (!uri.host()) {\n                uri.host(this.uri.host());\n            } else if (uri.host() != this.uri.host()) {\n                log.silly('[crawler] Skip: different host - ' + url);\n                continue;\n            }\n\n            // Force protocol to same as this.uri\n            uri.protocol(this.uri.protocol());\n\n            // Normalize\n            uri.normalize();\n\n            // Remove trailing slash\n            uri.path(uri.path().replace(/\\/$/, \"\"));\n\n            // Remove #anchor\n            uri.hash('');\n\n            // Build url\n            let url = uri.toString();\n\n            // Skip handled links\n            if (this.cache.has(url)) {\n                log.silly('[crawler] Skip: found in cache - ' + url);\n                continue;\n            } else {\n                this.cache.set(url);\n            }\n\n            // Skip certain file types\n            if (uri.suffix().match(this.skipFiles) !== null) {\n                log.silly('[crawler] Skip: file - ' + url);\n                continue;\n            }\n\n            // Check robots.txt\n            if (this.robots && this.robots.isDisallowed(url, USER_AGENT)) {\n                log.silly('[crawler] Skip: disallowed in robots.txt - ' + url);\n                continue;\n            }\n\n            // Skip urls in config\n            if (this.skip.path) {\n                let skip = false;\n                for (let p of this.skip.path) {\n                    if (uri.path().indexOf(p) === 0) {\n                        log.silly('[crawler] Skip: path - ' + url);\n                        skip = true;\n                        continue;\n                    }\n                }\n                if (skip) { continue; }\n            }\n            result.push(url);\n        };\n        return result;\n    }\n\n}\n"]}