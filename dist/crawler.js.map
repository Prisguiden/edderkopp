{"version":3,"sources":["../src/crawler.js"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;;;;AACA;;;;AACA;;;;AAEA;;;;AACA;;;;AACA;;;;AACA;;;;AACA;;;;AACA;;;;;;AAEA;;;;AAMI,oBAAY,GAAZ,EAA+B;AAAA,YAAd,OAAc,yDAAJ,EAAI;AAAA;;AAClB;;AAET;AAH2B;;AAAA,cAF/B,SAE+B,GAFnB,kJAEmB;AAI3B,cAAK,IAAL,GAAY,cAAE,KAAF,CAAQ,GAAR,EAAa,IAAb,EAAmB,IAAnB,CAAZ;;AAEA;AACA,cAAK,MAAL,GAAc,oBAAU,EAAE,UAAU,QAAQ,QAApB,EAA8B,UAAU,QAAQ,QAAhD,EAAV,CAAd;;AAEA;AACA,cAAK,OAAL,GAAe,sBAAf;;AAEA;AACA,cAAK,MAAL,GAAc,qBAAd;;AAEA;AACA,YAAI,QAAQ,KAAZ,EAAmB;AACf,kBAAK,GAAL,GAAW,wBAAX;AACH;;AAED;AACA,cAAK,MAAL,GAAc,QAAQ,KAAR,KAAkB,SAAlB,GAA8B,QAAQ,KAAtC,GAA8C,CAA5D,CArB2B,CAqBoC;AAC/D,cAAK,SAAL,GAAiB,QAAQ,QAAR,KAAqB,SAArB,GAAiC,QAAQ,QAAzC,GAAoD,CAArE;AACA,cAAK,SAAL,GAAiB,QAAQ,QAAR,KAAqB,SAArB,GAAiC,QAAQ,QAAzC,GAAoD,CAArE;AACA,cAAK,SAAL,GAAiB,QAAQ,QAAzB;AAxB2B;AAyB9B;;AAED;;;AA9BA;;;;;;;oBA+BY,M,yDAAS,E;;;;;;AACjB,8CAAI,KAAJ,CAAU,iBAAV;AACA,8CAAI,KAAJ,CAAU,MAAV;;AAEA;;uCACM,KAAK,MAAL,E;;;;AAEN;AACA,qCAAK,KAAL,GAAa,OAAO,IAApB;AACA,qCAAK,KAAL,GAAa,OAAO,IAAP,IAAe,EAA5B;AACA,qCAAK,OAAL,GAAe,OAAO,MAAtB;AACA,qCAAK,KAAL,GAAa,OAAO,IAApB;AACA,qCAAK,KAAL,GAAa,OAAO,IAApB;AACA,qCAAK,IAAL,GAAY,OAAO,GAAnB;;AAEI,mC,GAAM,cAAE,OAAF,CAAU,KAAK,IAAf,EAAqB,KAAK,KAA1B,C;;AAEV;;AACA,qCAAK,MAAL,CAAY,IAAZ;AACA,qCAAK,MAAL,CAAY,GAAZ,CAAgB,GAAhB;;AAEA;AACA,qCAAK,MAAL,CAAY,IAAZ;AACA,qCAAK,MAAL,CAAY,GAAZ,CAAgB,GAAhB;;AAEA;AACA,qCAAK,KAAL,GAAa,KAAb;;AAEA;iEACO,KAAK,MAAL,E;;;;;;;;;;;;;;;;;AAGX;;;;;;;;;;;AAEU,mC,GAAM,KAAK,MAAL,CAAY,GAAZ,E;;qCACR,G;;;;;;uCACiB,KAAK,QAAL,CAAc,GAAd,C;;;AAAb,oC;;AACJ,oCAAI,IAAJ,EAAU;AACN,yCAAK,OAAL,CAAa,IAAb,GAAoB,IAApB;;AAEA;AACI,yCAJE,GAIM,KAAK,SAAL,EAJN;;AAKN,wCAAI,KAAJ,EAAW;AACP,6CAAK,MAAL,CAAY,GAAZ,CAAgB,KAAhB;AACH;;AAED;AACI,wCAVE,GAUK,KAAK,QAAL,EAVL;;AAWN,wCAAI,IAAJ,EAAU;AACN,6CAAK,IAAL,CAAU,YAAV,EAAwB,IAAxB;AACH;AACJ;;qCACG,KAAK,MAAL,CAAY,K;;;;;AACZ,8CAAI,KAAJ,CAAU,gBAAV;;;;kEAGO,KAAK,MAAL,E;;;;;;;;;;;;;;;;;AAKnB;;;;;qGACe,G;;;;;;;;AACP,oC,GAAO,KAAK,GAAL,IAAY,CAAC,KAAK,SAAlB,GAA8B,KAAK,GAAL,CAAS,GAAT,CAAa,GAAb,CAA9B,GAAkD,K;;sCACzD,SAAS,K;;;;;AACT,8CAAI,OAAJ,CAAY,4BAAZ,EAA0C,KAAK,MAAL,CAAY,KAAtD,EAA6D,GAA7D;AACA,oCAAI,SAAS,IAAb,EAAmB;AAAE;AACjB,kDAAI,KAAJ,CAAU,gBAAV;AACH;;;;;qCAGG,KAAK,K;;;;;AACL,8CAAI,KAAJ,CAAU,sBAAV,EAAkC,KAAK,MAAvC;;uCACM,sBAAY;AAAA,2CAAW,WAAW,OAAX,EAAoB,OAAK,MAAL,GAAc,IAAlC,CAAX;AAAA,iCAAZ,C;;;;;;;AAEN,qCAAK,KAAL,GAAa,IAAb;;;;AAGJ;AACA,8CAAI,OAAJ,CAAY,kBAAZ,EAAgC,KAAK,MAAL,CAAY,KAA5C,EAAmD,GAAnD;;;uCAEoB,wBAAS,GAAT,C;;;AAAZ,mC;;AACJ,8CAAI,KAAJ,CAAU,IAAI,OAAd;AACA,8CAAI,KAAJ,CAAU,yBAAV,EAAqC,IAAI,IAAzC,EAA+C,IAAI,IAAJ,GAAW,MAAX,GAAoB,cAAnE;AACA,8CAAI,KAAJ,CAAU,qBAAqB,IAAI,IAAnC;AACA,uCAAO,IAAI,IAAX;;;;;;;;AAEA,8CAAI,KAAJ;AACA,uCAAO,IAAP;;;AAEJ,oCAAI,KAAK,GAAT,EAAc;AACV,yCAAK,GAAL,CAAS,GAAT,CAAa,GAAb,EAAkB,IAAlB;AACH;;;kEAEE,I;;;;;;;;;;;;;;;;;AAGX;;;;oCACY;AACR;AACA,gBAAI,WAAW,IAAf;AACA,gBAAI,KAAK,KAAL,IAAc,OAAlB,EAA2B;AACvB,2BAAW,KAAX;AACH,aAFD,MAEO,IAAI,KAAK,KAAL,IAAc,WAAlB,EAA+B;AAClC,oBAAI,QAAQ,KAAK,MAAL,CAAY,KAAZ,GAAoB,CAAhC;AACA,oBAAI,SAAS,KAAK,OAAL,CAAa,MAA1B,EAAkC;AAC9B,+BAAW,KAAX;AACH;AACJ;;AAED;AACA,gBAAI,cAAJ;AACA,gBAAI,QAAJ,EAAc;;AAEV;AACA,oBAAI,eAAJ;AACA,oBAAI,KAAK,KAAL,IAAc,WAAlB,EAA+B;AAC3B,6BAAS,KAAK,OAAL,CAAa,KAAb,CAAT;AACH,iBAFD,MAEO,IAAI,KAAK,OAAT,EAAkB;AACrB,6BAAS,KAAK,OAAd;AACH;;AAED;AACA,wBAAQ,KAAK,OAAL,CAAa,QAAb,CAAsB,MAAtB,EAA8B,KAAK,KAAnC,CAAR;AACA,8BAAI,KAAJ,CAAU,0BAAV,EAAsC,MAAM,MAA5C;;AAEA;AACA,wBAAQ,KAAK,cAAL,CAAoB,KAApB,CAAR;AACA,8BAAI,KAAJ,CAAU,sCAAV,EAAkD,MAAM,MAAxD;AACH;;AAED,mBAAO,SAAS,MAAM,MAAf,GAAwB,KAAxB,GAAgC,KAAvC;AACH;;AAED;;;;mCACW;AACP;AACA,gBAAI,UAAU,KAAd;AACA,gBAAI,KAAK,KAAL,IAAc,WAAlB,EAA+B;AAC3B,oBAAI,KAAK,OAAL,CAAa,MAAb,GAAsB,CAAtB,KAA4B,KAAK,MAAL,CAAY,KAA5C,EAAmD;AAC/C,8BAAU,IAAV;AACH;AACJ,aAJD,MAIO,IAAI,KAAK,KAAT,EAAgB;AACnB,oBAAI,OAAO,KAAK,KAAZ,KAAsB,QAA1B,EAAoC;AAChC,wBAAI,IAAI,KAAJ,CAAU,IAAI,MAAJ,CAAW,KAAK,KAAhB,CAAV,CAAJ,EAAuC;AACnC,kCAAU,IAAV;AACH;AACJ,iBAJD,MAIO;AACH,wBAAI,KAAK,OAAL,CAAa,IAAb,CAAkB,KAAK,KAAL,CAAW,IAA7B,CAAJ,EAAwC;AACpC,kCAAU,IAAV;AACH;AACJ;AACJ,aAVM,MAUA;AACH,0BAAU,IAAV;AACH;;AAED,gBAAI,aAAJ;AACA,gBAAI,OAAJ,EAAa;AACT;AACA,oBAAI,KAAK,IAAT,EAAe;AACX,2BAAO,EAAP;AACA,yBAAK,IAAI,IAAT,IAAiB,KAAK,IAAtB,EAA4B;AACxB,6BAAK,IAAL,IAAa,KAAK,OAAL,CAAa,OAAb,CAAqB,KAAK,IAAL,CAAU,IAAV,CAArB,CAAb;AACH;AACJ,iBALD,MAKO;AACH,2BAAO,KAAK,OAAL,CAAa,IAApB;AACH;AACJ,aAVD,MAUO;AACH,uBAAO,KAAP;AACH;;AAED,mBAAO,IAAP;AACH;;AAED;;;;uCACe,K,EAAO;AAClB,gBAAI,SAAS,EAAb;AACA;AAFkB;AAAA;AAAA;;AAAA;AAGlB,gEAAiB,KAAjB,4GAAwB;AAAA,wBAAf,IAAe;;AACpB;AACA,wBAAI,OAAM,cAAE,KAAF,CAAQ,IAAR,EAAc,KAAd,EAAqB,IAArB,CAAV,CAFoB,CAEkB;;AAEtC;AACA,wBAAI,KAAI,QAAJ,IAAgB,KAAI,QAAJ,CAAa,OAAb,CAAqB,MAArB,MAAiC,CAArD,EAAwD;AACpD,sCAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,yBAAtC;AACA;AACH;;AAED,wBAAI,CAAC,KAAI,QAAT,EAAmB;AACf;AACA,6BAAI,QAAJ,GAAe,KAAK,IAAL,CAAU,QAAzB;AACH,qBAHD,MAGO,IAAI,KAAI,QAAJ,IAAgB,KAAK,IAAL,CAAU,QAA9B,EAAwC;AAC3C;AACA,sCAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,mBAAtC;AACA;AACH;;AAED,wBAAI,KAAI,QAAR,EAAkB;AACd;AACA,4BAAI,UAAU,KAAI,QAAJ,CAAa,KAAb,CAAmB,cAAnB,CAAd;AACA,4BAAI,OAAJ,EAAa;AACT,gCAAI,QAAQ,CAAR,EAAW,KAAX,CAAiB,KAAK,UAAtB,MAAsC,IAA1C,EAAgD;AAC5C,8CAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,cAAtC;AACA;AACH;AACJ;;AAED;AACA,6BAAI,QAAJ,GAAe,KAAI,QAAJ,CAAa,OAAb,CAAqB,KAArB,EAA4B,EAA5B,CAAf;AACH;;AAED;AACA,yBAAI,QAAJ,GAAe,KAAK,IAAL,CAAU,QAAzB;;AAEA;AACA,yBAAI,IAAJ,GAAW,IAAX;;AAEA;AACA,wBAAI,YAAY,KAAI,MAAJ,EAAhB;;AAEA;AACA,wBAAI,KAAK,MAAL,CAAY,GAAZ,CAAgB,SAAhB,CAAJ,EAAgC;AAC5B,sCAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,mBAAtC;AACA;AACH,qBAHD,MAGO;AACH,6BAAK,MAAL,CAAY,GAAZ,CAAgB,SAAhB;AACH;;AAED;AACA,wBAAI,KAAK,OAAL,IAAgB,KAAK,OAAL,CAAa,YAAb,CAA0B,SAA1B,EAAqC,UAArC,CAApB,EAAsE;AAClE,sCAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,6BAAtC;AACA;AACH;;AAED,kCAAI,KAAJ,CAAU,qBAAqB,IAA/B;AACA,2BAAO,IAAP,CAAY,SAAZ;AACH;AA7DiB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AA6DjB;AACD,mBAAO,MAAP;AACH;;AAED;;;;;;;;;;;qCAEQ,KAAK,O;;;;;;;;AAIH,mC,GAAM,KAAK,IAAL,CAAU,MAAV,KAAqB,Y;AAC7B,uC,GAAU,KAAK,GAAL,IAAY,CAAC,KAAK,SAAlB,GAA8B,KAAK,GAAL,CAAS,GAAT,CAAa,GAAb,CAA9B,GAAkD,K;;sCAC5D,YAAY,K;;;;;AACZ,8CAAI,OAAJ,CAAY,uBAAZ,EAAqC,GAArC;AACA,oCAAI,YAAY,IAAhB,EAAsB;AAClB,kDAAI,IAAJ,CAAS,eAAT;AACH;;;;;AAED,8CAAI,OAAJ,CAAY,eAAe,GAA3B;;;uCAEoB,wBAAS,GAAT,C;;;AAAZ,mC;;AACJ,0CAAU,IAAI,IAAd;;;;;;;;AAEA,8CAAI,KAAJ;AACA,0CAAU,IAAV;;;AAEJ,oCAAI,KAAK,GAAT,EAAc;AACV,yCAAK,GAAL,CAAS,GAAT,CAAa,GAAb,EAAkB,OAAlB;AACH;;;qCAGD,O;;;;;AACA,qCAAK,OAAL,GAAe,4BAAa,GAAb,EAAkB,OAAlB,CAAf;;AAEA;AACM,qC,GAAQ,KAAK,OAAL,CAAa,aAAb,E;;AACd,oCAAI,SAAS,QAAQ,KAAK,MAA1B,EAAkC;AAC9B,yCAAK,MAAL,GAAc,KAAd;AACH;;AAED;;oCACK,KAAK,OAAL,CAAa,SAAb,CAAuB,KAAK,IAAL,CAAU,MAAV,EAAvB,EAA2C,UAA3C,C;;;;;sCACK,IAAI,KAAJ,CAAU,sCAAV,C","file":"crawler.js","sourcesContent":["import EventEmitter from 'events';\nimport u from 'url';\nimport robotsParser from 'robots-parser';\n\nimport log from './log';\nimport download from \"./download\";\nimport Parser from \"./parser\";\nimport Queue from './queue';\nimport WebCache from './web-cache';\nimport Cache from './cache';\n\n// Crawler\nexport default class extends EventEmitter {\n\n    // Skip some common filetypes 'cause you never know whats out there (http://fileinfo.com/filetypes/common)\n    skipFiles = /jpg|jpeg|png|gif|bmp|tif|tiff|svg|pdf|wav|mpa|mp3|avi|flv|m4v|mov|mp4|mpg|swf|wmv|tar|gz|zip|rar|pkg|7z|xls|doc|log|odt|rtf|txt|exe|jar|com|bat/i;\n\n    constructor(url, options = {}) {\n        super(); // must\n\n        // Set root url\n        this._url = u.parse(url, true, true);\n\n        // Use Queue to handle links\n        this._queue = new Queue({ maxItems: options.maxItems, maxDepth: options.maxDepth });\n\n        // Use Parser to get links and data\n        this._parser = new Parser();\n\n        // Use Cache to not handle an url more than once\n        this._cache = new Cache();\n\n        // Use WebCache to cache html and save unnecessary downloads\n        if (options.cache) {\n            this._wc = new WebCache();\n        }\n\n        // Handle options\n        this._delay = options.delay !== undefined ? options.delay : 5; // seconds\n        this._maxItems = options.maxItems !== undefined ? options.maxItems : 5;\n        this._maxDepth = options.maxDepth !== undefined ? options.maxDepth : 2;\n        this._download = options.download;\n    }\n\n    // Start crawling!\n    async start(target = {}) {\n        log.debug('[crawler] start');\n        log.silly(target);\n\n        // Handle robots.txt\n        await this._robot();\n\n        // Handle target\n        this._mode = target.mode;\n        this._path = target.path || '';\n        this._follow = target.follow;\n        this._skip = target.skip;\n        this._find = target.find;\n        this._get = target.get;\n\n        let url = u.resolve(this._url, this._path);\n\n        // Init queue and add entry point\n        this._queue.init();\n        this._queue.add(url);\n\n        // Init cache and set entry point\n        this._cache.init();\n        this._cache.set(url);\n\n        // Don't wait on first download\n        this._wait = false;\n\n        // Start crawling from queue\n        return this._crawl();\n    }\n\n    // Recursive crawl urls from queue until queue is empty\n    async _crawl() {\n        const url = this._queue.get();\n        if (url) {\n            let html = await this._getHtml(url);\n            if (html) {\n                this._parser.html = html;\n\n                // Get links and add to queue\n                let links = this._getLinks();\n                if (links) {\n                    this._queue.add(links);\n                }\n\n                // Get data and tell 'found-data' listeners about it\n                let data = this._getData();\n                if (data) {\n                    this.emit('found-data', data);\n                }\n            }\n            if (this._queue.empty) {\n                log.debug('[crawler] done');\n                return;\n            } else {\n                return this._crawl();\n            }\n        }\n    }\n\n    // Get html from cache or download\n    async _getHtml(url) {\n        let html = this._wc && !this._download ? this._wc.get(url) : false;\n        if (html !== false) {\n            log.verbose('[crawler] %s: %s - CACHED ', this._queue.depth, url);\n            if (html === null) { // if 404..\n                log.error('No html (404?)');\n            }\n        } else {\n            // Wait between each download, but not the first\n            if (this._wait) {\n                log.debug('[crawler] sleep %s s', this._delay);\n                await new Promise(resolve => setTimeout(resolve, this._delay * 1000));\n            } else {\n                this._wait = true;\n            }\n\n            // Download\n            log.verbose('[crawler] %s: %s', this._queue.depth, url);\n            try {\n                var res = await download(url);\n                log.silly(res.headers);\n                log.debug('[crawler] size: %s (%s)', res.size, res.gzip ? 'gzip' : 'uncompressed');\n                log.debug('[crawler] time: ' + res.time);\n                html = res.html;\n            } catch (err) {\n                log.error(err);\n                html = null;\n            }\n            if (this._wc) {\n                this._wc.set(url, html);\n            }\n        }\n        return html;\n    }\n\n    // Get links for different modes\n    _getLinks() {\n        // Continue crawling?\n        let getLinks = true;\n        if (this._mode == 'fetch') {\n            getLinks = false;\n        } else if (this._mode == 'waterfall') {\n            var index = this._queue.depth - 1;\n            if (index == this._follow.length) {\n                getLinks = false;\n            }\n        }\n\n        // Get links to crawl\n        let links;\n        if (getLinks) {\n\n            // Handle follow rules\n            let follow;\n            if (this._mode == 'waterfall') {\n                follow = this._follow[index];\n            } else if (this._follow) {\n                follow = this._follow;\n            }\n\n            // Get links\n            links = this._parser.getLinks(follow, this._skip);\n            log.debug('[crawler] %d links found', links.length);\n\n            // Validate links\n            links = this._validateLinks(links);\n            log.debug('[crawler] %d links passed validation', links.length);\n        }\n\n        return links && links.length ? links : false;\n    }\n\n    // Get data by parsing html\n    _getData() {\n        // Get data? Go through cases..\n        let getData = false;\n        if (this._mode == 'waterfall') {\n            if (this._follow.length + 1 === this._queue.depth) {\n                getData = true;\n            }\n        } else if (this._find) {\n            if (typeof this._find === 'string') {\n                if (url.match(new RegExp(this._find))) {\n                    getData = true;\n                }\n            } else {\n                if (this._parser.find(this._find.elem)) {\n                    getData = true;\n                }\n            }\n        } else {\n            getData = true;\n        }\n\n        let data;\n        if (getData) {\n            // Return parsed html if \"get\" is defined in config, else plain html\n            if (this._get) {\n                data = {};\n                for (let prop in this._get) {\n                    data[prop] = this._parser.getData(this._get[prop])\n                }\n            } else {\n                data = this._parser.html;\n            }\n        } else {\n            data = false;\n        }\n\n        return data;\n    }\n\n    // Validate links\n    _validateLinks(links) {\n        let result = [];\n        // let uri = new URI();\n        for (let link of links) {\n            // Populate url object\n            let url = u.parse(link, false, true); // https://nodejs.org/api/url.html#url_url_parse_urlstring_parsequerystring_slashesdenotehost\n\n            // Skip protocols other than http(s) (mailto, ftp, ..)\n            if (url.protocol && url.protocol.indexOf('http') !== 0) {\n                log.silly('[crawler] Skip: ' + link + ' (unsupported protocol)');\n                continue;\n            }\n\n            if (!url.hostname) {\n                // Set host if empty\n                url.hostname = this._url.hostname;\n            } else if (url.hostname != this._url.hostname) {\n                // Skip different/external hosts\n                log.silly('[crawler] Skip: ' + link + ' (different host)');\n                continue;\n            }\n\n            if (url.pathname) {\n                // Skip certain file types\n                let matches = url.pathname.match(/\\.(\\w{2,4})$/);\n                if (matches) {\n                    if (matches[1].match(this._skipFiles) !== null) {\n                        log.silly('[crawler] Skip: ' + link + ' (file type)');\n                        continue;\n                    }\n                }\n\n                // Remove trailing slash (questionable, this can be improved?)\n                url.pathname = url.pathname.replace(/\\/$/, '');\n            }\n\n            // Force protocol to same as this._url\n            url.protocol = this._url.protocol;\n\n            // Remove #hash\n            url.hash = null;\n\n            // Build url\n            let urlString = url.format();\n\n            // Skip handled links\n            if (this._cache.has(urlString)) {\n                log.silly('[crawler] Skip: ' + link + ' (found in cache)');\n                continue;\n            } else {\n                this._cache.set(urlString);\n            }\n\n            // Check robots.txt\n            if (this._robots && this._robots.isDisallowed(urlString, USER_AGENT)) {\n                log.silly('[crawler] Skip: ' + link + ' (disallowed in robots.txt)');\n                continue;\n            }\n\n            log.silly('[crawler] New:  ' + link);\n            result.push(urlString);\n        };\n        return result;\n    }\n\n    // Handle robots.txt\n    async _robot() {\n        if (this._robots) {\n            return;\n        }\n\n        const url = this._url.format() + 'robots.txt';\n        let content = this._wc && !this._download ? this._wc.get(url) : false;\n        if (content !== false) {\n            log.verbose('[crawler] %s - CACHED', url);\n            if (content === null) {\n                log.warn('No robots.txt');\n            }\n        } else {\n            log.verbose('[crawler] ' + url);\n            try {\n                var res = await download(url);\n                content = res.html;\n            } catch (err) {\n                log.error(err);\n                content = null;\n            }\n            if (this._wc) {\n                this._wc.set(url, content);\n            }\n        }\n\n        if (content) {\n            this._robots = robotsParser(url, content);\n\n            // If robots spesifies delay and it is greater than ours, respect it!\n            const delay = this._robots.getCrawlDelay();\n            if (delay && delay > this._delay) {\n                this._delay = delay;\n            }\n\n            // Makes sure we are wanted\n            if (!this._robots.isAllowed(this._url.format(), USER_AGENT)) {\n                throw new Error('User-Agent not allowed by robots.txt')\n            }\n        }\n    }\n\n}\n"]}