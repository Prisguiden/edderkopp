{"version":3,"sources":["../src/crawler.js"],"names":[],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAA;;;;AACA;;;;AACA;;;;AAEA;;;;AACA;;;;AACA;;;;AACA;;;;AACA;;;;;;AAEA;;;;AAMI,oBAAY,GAAZ,EAA+B;AAAA,YAAd,OAAc,yDAAJ,EAAI;AAAA;;AAClB;;AAET;AAH2B;;AAAA,cAF/B,UAE+B,GAFlB,kJAEkB;AAI3B,cAAK,IAAL,GAAY,cAAE,KAAF,CAAQ,GAAR,EAAa,IAAb,EAAmB,IAAnB,CAAZ;;AAEA;AACA,cAAK,MAAL,GAAc,QAAQ,KAAR,KAAkB,SAAlB,GAA8B,QAAQ,KAAtC,GAA8C,CAA5D,CAP2B,CAOoC;;AAE/D;AACA,cAAK,MAAL,GAAc,oBAAU,EAAE,UAAU,QAAQ,QAApB,EAA8B,UAAU,QAAQ,QAAhD,EAAV,CAAd;;AAEA;AACA,cAAK,OAAL,GAAe,sBAAf;;AAEA;AACA,cAAK,MAAL,GAAc,qBAAd;;AAhB2B;AAkB9B;;AAED;;;AAvBA;;;;;;;oBAwBY,M,yDAAS,E;;;;;;AACjB,8CAAI,KAAJ,CAAU,iBAAV;AACA,8CAAI,KAAJ,CAAU,MAAV;;AAEA;;;uCAEU,KAAK,MAAL,E;;;;;;;;;iEAEC,kBAAQ,MAAR,a;;;;AAGX;AACA,mDAAS,KAAT,GAAiB,KAAK,MAAtB;;AAEA;AACA,qCAAK,KAAL,GAAa,OAAO,IAApB;AACA,qCAAK,KAAL,GAAa,OAAO,IAAP,IAAe,EAA5B;AACA,qCAAK,KAAL,GAAa,OAAO,IAApB;AACA,qCAAK,KAAL,GAAa,OAAO,IAApB;AACA,qCAAK,KAAL,GAAa,OAAO,IAApB;AACA,qCAAK,KAAL,GAAa,OAAO,IAApB;;AAEI,mC,GAAM,KAAK,IAAL,CAAU,QAAV,GAAqB,IAArB,GAA4B,KAAK,IAAL,CAAU,QAAtC,GAAiD,KAAK,K;;AAEhE;;AACA,qCAAK,MAAL,CAAY,IAAZ;AACA,qCAAK,MAAL,CAAY,GAAZ,CAAgB,GAAhB;;AAEA;AACA,qCAAK,MAAL,CAAY,IAAZ;AACA,qCAAK,MAAL,CAAY,GAAZ,CAAgB,GAAhB;;AAEA;iEACO,KAAK,MAAL,E;;;;;;;;;;;;;;;;;AAGX;;;;;;;;;;;AAEU,mC,GAAM,KAAK,MAAL,CAAY,GAAZ,E;;qCACR,G;;;;;AACA;AACA,oCAAI,KAAK,MAAL,IAAe,KAAK,MAAL,CAAY,KAA/B,EAAsC;AAClC,yCAAK,MAAL,GAAc,KAAK,MAAL,CAAY,KAA1B;AACA,kDAAI,OAAJ,CAAY,4BAAZ,EAA0C,KAAK,MAAL,CAAY,KAAtD;AACH;;AAED;AACI,uC,GAAU,I;;;uCAEM,mBAAS,GAAT,CAAa,GAAb,C;;;AAAhB,uC;;;;;;;;AAEA,8CAAI,KAAJ;;;;AAGJ;AACA,oCAAI,OAAJ,EAAa;AACT,yCAAK,OAAL,CAAa,IAAb,GAAoB,OAApB;;AAEA;AACI,yCAJK,GAIG,KAAK,SAAL,EAJH;;AAKT,wCAAI,KAAJ,EAAW;AACP,6CAAK,MAAL,CAAY,GAAZ,CAAgB,KAAhB;AACH;;AAED;AACI,wCAVK,GAUE,KAAK,QAAL,EAVF;;AAWT,wCAAI,IAAJ,EAAU;AACN,6CAAK,IAAL,CAAU,aAAV,EAAyB,IAAzB;AACH;AACJ;;AAED;;qCACI,KAAK,MAAL,CAAY,K;;;;;AACZ,8CAAI,KAAJ,CAAU,gBAAV;;;;kEAGO,KAAK,MAAL,E;;;;;;;;;;;;;;;;;AAKnB;;;;oCACY;AACR;AACA,gBAAI,WAAW,IAAf;AACA,gBAAI,KAAK,KAAL,IAAc,OAAlB,EAA2B;AACvB,2BAAW,KAAX;AACH,aAFD,MAEO,IAAI,KAAK,KAAL,IAAc,WAAlB,EAA+B;AAClC,oBAAI,QAAQ,KAAK,MAAL,CAAY,KAAZ,GAAoB,CAAhC;AACA,oBAAI,SAAS,KAAK,KAAL,CAAW,MAAxB,EAAgC;AAC5B,+BAAW,KAAX;AACH;AACJ;;AAED;AACA,gBAAI,QAAQ,IAAZ;AACA,gBAAI,QAAJ,EAAc;;AAEV;AACA,oBAAI,aAAJ;AACA,oBAAI,KAAK,KAAL,IAAc,WAAlB,EAA+B;AAC3B,2BAAO,KAAK,KAAL,CAAW,KAAX,CAAP;AACH,iBAFD,MAEO,IAAI,KAAK,KAAT,EAAgB;AACnB,2BAAO,KAAK,KAAZ;AACH;;AAED;AACA,wBAAQ,KAAK,OAAL,CAAa,QAAb,CAAsB,IAAtB,EAA4B,KAAK,KAAjC,CAAR;AACA,8BAAI,KAAJ,CAAU,0BAAV,EAAsC,MAAM,MAA5C;;AAEA;AACA,wBAAQ,KAAK,cAAL,CAAoB,KAApB,CAAR;AACA,8BAAI,KAAJ,CAAU,sCAAV,EAAkD,MAAM,MAAxD;AACH;;AAED,mBAAO,KAAP;AACH;;AAED;;;;mCACW;AACP;AACA,gBAAI,UAAU,KAAd;AACA,gBAAI,KAAK,KAAL,IAAc,WAAlB,EAA+B;AAC3B,oBAAI,KAAK,KAAL,CAAW,MAAX,GAAoB,CAApB,KAA0B,KAAK,MAAL,CAAY,KAA1C,EAAiD;AAC7C,8BAAU,IAAV;AACH;AACJ,aAJD,MAIO,IAAI,KAAK,KAAT,EAAgB;AACnB,oBAAI,OAAO,KAAK,KAAZ,KAAsB,QAA1B,EAAoC;AAChC,wBAAI,IAAI,KAAJ,CAAU,IAAI,MAAJ,CAAW,KAAK,KAAhB,CAAV,CAAJ,EAAuC;AACnC,kCAAU,IAAV;AACH;AACJ,iBAJD,MAIO;AACH,wBAAI,KAAK,OAAL,CAAa,IAAb,CAAkB,KAAK,KAAL,CAAW,IAA7B,CAAJ,EAAwC;AACpC,kCAAU,IAAV;AACH;AACJ;AACJ,aAVM,MAUA;AACH,0BAAU,IAAV;AACH;;AAED,gBAAI,aAAJ;AACA,gBAAI,OAAJ,EAAa;AACT;AACA,oBAAI,KAAK,KAAT,EAAgB;AACZ,2BAAO,EAAP;AACA,yBAAK,IAAI,IAAT,IAAiB,KAAK,KAAtB,EAA6B;AACzB,6BAAK,IAAL,IAAa,KAAK,OAAL,CAAa,OAAb,CAAqB,KAAK,KAAL,CAAW,IAAX,CAArB,CAAb;AACH;AACJ,iBALD,MAKO;AACH,2BAAO,KAAK,OAAL,CAAa,IAApB;AACH;AACJ,aAVD,MAUO;AACH,uBAAO,KAAP;AACH;;AAED,mBAAO,IAAP;AACH;;AAED;;;;uCACe,K,EAAO;AAClB,gBAAI,SAAS,EAAb;AACA;AAFkB;AAAA;AAAA;;AAAA;AAGlB,gEAAiB,KAAjB,4GAAwB;AAAA,wBAAf,IAAe;;AACpB;AACA,wBAAI,OAAM,cAAE,KAAF,CAAQ,IAAR,EAAc,KAAd,EAAqB,IAArB,CAAV,CAFoB,CAEkB;;AAEtC;AACA,wBAAI,KAAI,QAAJ,IAAgB,KAAI,QAAJ,CAAa,OAAb,CAAqB,MAArB,MAAiC,CAArD,EAAwD;AACpD,sCAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,yBAAtC;AACA;AACH;;AAED,wBAAI,CAAC,KAAI,QAAT,EAAmB;AACf;AACA,6BAAI,QAAJ,GAAe,KAAK,IAAL,CAAU,QAAzB;AACH,qBAHD,MAGO,IAAI,KAAI,QAAJ,IAAgB,KAAK,IAAL,CAAU,QAA9B,EAAwC;AAC3C;AACA,sCAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,mBAAtC;AACA;AACH;;AAED,wBAAI,KAAI,QAAR,EAAkB;AACd;AACA,4BAAI,UAAU,KAAI,QAAJ,CAAa,KAAb,CAAmB,cAAnB,CAAd;AACA,4BAAI,OAAJ,EAAa;AACT,gCAAI,QAAQ,CAAR,EAAW,KAAX,CAAiB,KAAK,UAAtB,MAAsC,IAA1C,EAAgD;AAC5C,8CAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,cAAtC;AACA;AACH;AACJ;;AAED;AACA,6BAAI,QAAJ,GAAe,KAAI,QAAJ,CAAa,OAAb,CAAqB,KAArB,EAA4B,EAA5B,CAAf;AACH;;AAED;AACA,yBAAI,QAAJ,GAAe,KAAK,IAAL,CAAU,QAAzB;;AAEA;AACA,yBAAI,IAAJ,GAAW,IAAX;;AAEA;AACA,wBAAI,YAAY,KAAI,MAAJ,EAAhB;;AAEA;AACA,wBAAI,KAAK,MAAL,CAAY,GAAZ,CAAgB,SAAhB,CAAJ,EAAgC;AAC5B,sCAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,mBAAtC;AACA;AACH,qBAHD,MAGO;AACH,6BAAK,MAAL,CAAY,GAAZ,CAAgB,SAAhB;AACH;;AAED;AACA,wBAAI,KAAK,OAAL,IAAgB,KAAK,OAAL,CAAa,YAAb,CAA0B,SAA1B,EAAqC,UAArC,CAApB,EAAsE;AAClE,sCAAI,KAAJ,CAAU,qBAAqB,IAArB,GAA4B,6BAAtC;AACA;AACH;;AAED,kCAAI,KAAJ,CAAU,qBAAqB,IAA/B;AACA,2BAAO,IAAP,CAAY,SAAZ;AACH;AA7DiB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;AA6DjB;AACD,mBAAO,MAAP;AACH;;AAED;;;;;;;;;;;qCAEQ,KAAK,O;;;;;;;;AAIH,mC,GAAM,KAAK,IAAL,CAAU,MAAV,KAAqB,Y;AAE7B,uC,GAAU,I;;;uCAEM,mBAAS,GAAT,CAAa,GAAb,C;;;AAAhB,uC;;;;;;;;AAEA,8CAAI,KAAJ;;;qCAGA,O;;;;;AACA;AACA,qCAAK,OAAL,GAAe,4BAAa,GAAb,EAAkB,OAAlB,CAAf;;AAEA;;oCACK,KAAK,OAAL,CAAa,SAAb,CAAuB,KAAK,IAAL,CAAU,MAAV,EAAvB,EAA2C,UAA3C,C;;;;;sCACK,IAAI,KAAJ,CAAU,sCAAV,C;;;;AAGV;AACM,qC,GAAQ,KAAK,OAAL,CAAa,aAAb,E;;AACd,oCAAI,SAAS,QAAQ,KAAK,MAA1B,EAAkC;AAC9B,yCAAK,MAAL,GAAc,KAAd;AACH;;;;;AAED,8CAAI,KAAJ,CAAU,yBAAV","file":"crawler.js","sourcesContent":["import EventEmitter from 'events';\nimport u from 'url';\nimport robotsParser from 'robots-parser';\n\nimport log from './log';\nimport Download from \"./download\";\nimport Parser from \"./parser\";\nimport Queue from './queue';\nimport Cache from './cache';\n\n// Crawler\nexport default class extends EventEmitter {\n\n    // Skip some common filetypes 'cause you never know whats out there (http://fileinfo.com/filetypes/common)\n    _skipFiles = /jpg|jpeg|png|gif|bmp|tif|tiff|svg|pdf|wav|mpa|mp3|avi|flv|m4v|mov|mp4|mpg|swf|wmv|tar|gz|zip|rar|pkg|7z|xls|doc|log|odt|rtf|txt|exe|jar|com|bat/i;\n\n    constructor(url, options = {}) {\n        super(); // must\n\n        // Set root url\n        this._url = u.parse(url, true, true);\n\n        // Delay\n        this._delay = options.delay !== undefined ? options.delay : 5; // seconds\n\n        // Use Queue to handle links\n        this._queue = new Queue({ maxItems: options.maxItems, maxDepth: options.maxDepth });\n\n        // Use Parser to get links and data\n        this._parser = new Parser();\n\n        // Use Cache to not handle an url more than once\n        this._cache = new Cache();\n\n    }\n\n    // Start crawling!\n    async start(target = {}) {\n        log.debug('[crawler] start');\n        log.silly(target);\n\n        // Handle robots.txt\n        try {\n            await this._robot();\n        } catch (err) {\n            return Promise.reject(err);\n        }\n\n        // Handle delay\n        Download.delay = this._delay;\n\n        // Handle target\n        this._mode = target.mode;\n        this._path = target.path || '';\n        this._link = target.link;\n        this._skip = target.skip;\n        this._page = target.page;\n        this._data = target.data;\n\n        let url = this._url.protocol + '//' + this._url.hostname + this._path;\n\n        // Init queue and add entry point\n        this._queue.init();\n        this._queue.add(url);\n\n        // Init cache and set entry point\n        this._cache.init();\n        this._cache.set(url);\n\n        // Start crawling from queue\n        return this._crawl();\n    }\n\n    // Recursive crawl urls from queue until queue is empty\n    async _crawl() {\n        const url = this._queue.get();\n        if (url) {\n            // Depth\n            if (this._depth != this._queue.depth) {\n                this._depth = this._queue.depth;\n                log.verbose('[crawler] --- depth %s ---', this._queue.depth);\n            }\n\n            // Download\n            let content = null;\n            try {\n                content = await Download.get(url);\n            } catch (err) {\n                log.error(err);\n            }\n\n            // Get links and data\n            if (content) {\n                this._parser.html = content;\n\n                // Get links and add to queue\n                let links = this._getLinks();\n                if (links) {\n                    this._queue.add(links);\n                }\n\n                // Get data and tell 'handle-data' listeners about it\n                let data = this._getData();\n                if (data) {\n                    this.emit('handle-data', data);\n                }\n            }\n\n            // Check queue and continue or return\n            if (this._queue.empty) {\n                log.debug('[crawler] done');\n                return;\n            } else {\n                return this._crawl();\n            }\n        }\n    }\n\n    // Get links for different modes\n    _getLinks() {\n        // Continue crawling?\n        let getLinks = true;\n        if (this._mode == 'fetch') {\n            getLinks = false;\n        } else if (this._mode == 'waterfall') {\n            var index = this._queue.depth - 1;\n            if (index == this._link.length) {\n                getLinks = false;\n            }\n        }\n\n        // Get links to crawl\n        let links = null;\n        if (getLinks) {\n\n            // Handle link rules\n            let link;\n            if (this._mode == 'waterfall') {\n                link = this._link[index];\n            } else if (this._link) {\n                link = this._link;\n            }\n\n            // Get links\n            links = this._parser.getLinks(link, this._skip);\n            log.debug('[crawler] %d links found', links.length);\n\n            // Validate links\n            links = this._validateLinks(links);\n            log.debug('[crawler] %d links passed validation', links.length);\n        }\n\n        return links;\n    }\n\n    // Get data by parsing html\n    _getData() {\n        // Get data? Go through cases..\n        let getData = false;\n        if (this._mode == 'waterfall') {\n            if (this._link.length + 1 === this._queue.depth) {\n                getData = true;\n            }\n        } else if (this._page) {\n            if (typeof this._page === 'string') {\n                if (url.match(new RegExp(this._page))) {\n                    getData = true;\n                }\n            } else {\n                if (this._parser.find(this._page.elem)) {\n                    getData = true;\n                }\n            }\n        } else {\n            getData = true;\n        }\n\n        let data;\n        if (getData) {\n            // Return parsed html if \"get\" is defined in config, else plain html\n            if (this._data) {\n                data = {};\n                for (let prop in this._data) {\n                    data[prop] = this._parser.getData(this._data[prop])\n                }\n            } else {\n                data = this._parser.html;\n            }\n        } else {\n            data = false;\n        }\n\n        return data;\n    }\n\n    // Validate links\n    _validateLinks(links) {\n        let result = [];\n        // let uri = new URI();\n        for (let link of links) {\n            // Populate url object\n            let url = u.parse(link, false, true); // https://nodejs.org/api/url.html#url_url_parse_urlstring_parsequerystring_slashesdenotehost\n\n            // Skip protocols other than http(s) (mailto, ftp, ..)\n            if (url.protocol && url.protocol.indexOf('http') !== 0) {\n                log.silly('[crawler] Skip: ' + link + ' (unsupported protocol)');\n                continue;\n            }\n\n            if (!url.hostname) {\n                // Set host if empty\n                url.hostname = this._url.hostname;\n            } else if (url.hostname != this._url.hostname) {\n                // Skip different/external hosts\n                log.silly('[crawler] Skip: ' + link + ' (different host)');\n                continue;\n            }\n\n            if (url.pathname) {\n                // Skip certain file types\n                let matches = url.pathname.match(/\\.(\\w{2,4})$/);\n                if (matches) {\n                    if (matches[1].match(this._skipFiles) !== null) {\n                        log.silly('[crawler] Skip: ' + link + ' (file type)');\n                        continue;\n                    }\n                }\n\n                // Remove trailing slash (questionable, this can be improved?)\n                url.pathname = url.pathname.replace(/\\/$/, '');\n            }\n\n            // Force protocol to same as this._url\n            url.protocol = this._url.protocol;\n\n            // Remove #hash\n            url.hash = null;\n\n            // Build url\n            let urlString = url.format();\n\n            // Skip handled links\n            if (this._cache.has(urlString)) {\n                log.silly('[crawler] Skip: ' + link + ' (found in cache)');\n                continue;\n            } else {\n                this._cache.set(urlString);\n            }\n\n            // Check robots.txt\n            if (this._robots && this._robots.isDisallowed(urlString, USER_AGENT)) {\n                log.silly('[crawler] Skip: ' + link + ' (disallowed in robots.txt)');\n                continue;\n            }\n\n            log.silly('[crawler] New:  ' + link);\n            result.push(urlString);\n        };\n        return result;\n    }\n\n    // Handle robots.txt\n    async _robot() {\n        if (this._robots) {\n            return;\n        }\n\n        const url = this._url.format() + 'robots.txt';\n\n        let content = null;\n        try {\n            content = await Download.get(url);\n        } catch (err) {\n            log.error(err);\n        }\n\n        if (content) {\n            // Init robots parser\n            this._robots = robotsParser(url, content);\n\n            // Makes sure we are wanted\n            if (!this._robots.isAllowed(this._url.format(), USER_AGENT)) {\n                throw new Error('User-Agent not allowed by robots.txt')\n            }\n\n            // If robots spesifies delay and it is greater than ours, respect it!\n            const delay = this._robots.getCrawlDelay();\n            if (delay && delay > this._delay) {\n                this._delay = delay;\n            }\n        } else {\n            log.debug('[crawler] No robots.txt');\n        }\n    }\n\n}\n"]}